{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e333e2f8-445b-464e-a127-7f7bfbf04d16",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Textual Inversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb9249d-8c83-4d7f-965c-854ef42ebefc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d4cc81-73e4-4774-b109-ea6cb88f727f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import jsonargparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import yaml\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    DDPMScheduler,\n",
    "    LMSDiscreteScheduler,\n",
    "    UNet2DConditionModel,\n",
    "    get_scheduler,\n",
    ")\n",
    "from PIL import Image\n",
    "from torch import autocast\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms as tfms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device:{torch_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3345c208-a369-4dfe-ac34-6e68aa9f6674",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = jsonargparse.dict_to_namespace(\n",
    "    yaml.safe_load(\n",
    "        \"\"\"\n",
    "data:\n",
    " image_dir: ../../images/kali \n",
    " size: 512\n",
    " placeholder_token: <kali-dog>\n",
    " initializer_token: dog\n",
    " learnable_property: object\n",
    "optimization:\n",
    " learning_rate: 0.000125\n",
    " batch_size: 4\n",
    " batch_accum: 4\n",
    " num_train_steps: 5000\n",
    "architecture:\n",
    " model_name: runwayml/stable-diffusion-v1-5\n",
    " scheduler: CompVis/stable-diffusion-v1-4\n",
    " train_scheduler_type: DDPMScheduler\n",
    " inference_scheduler_type: LMSDiscreteScheduler\n",
    "embedding_save_path: kali_saved_embedding_{}.bin\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44b0e4b-4fc9-4f7c-aa4b-2bda5ce4509d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def add_token(placeholder_token, text_encoder, tokenizer, embed):\n",
    "    # Add placeholder_token to the tokenizer\n",
    "    num_added_tokens = tokenizer.add_tokens(config.data.placeholder_token)\n",
    "    if num_added_tokens == 0:\n",
    "        raise ValueError(\n",
    "            f\"The tokenizer already contains the token {placeholder_token}. Please pass a different\"\n",
    "            \"placeholder_token` that is not already in the tokenizer.\"\n",
    "        )\n",
    "\n",
    "    placeholder_token_id = tokenizer.convert_tokens_to_ids(placeholder_token)\n",
    "    text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "    token_embeds = text_encoder.get_input_embeddings().weight.data\n",
    "    \n",
    "    if isinstance(embed, int):\n",
    "        token_embeds[placeholder_token_id] = token_embeds[embed]\n",
    "    elif isinstance(embed, torch.Tensor):\n",
    "        token_embeds[placeholder_token_id] = embed\n",
    "\n",
    "    return placeholder_token_id\n",
    "\n",
    "\n",
    "def get_models(config, scheduler_type):\n",
    "    vae = AutoencoderKL.from_pretrained(config.architecture.model_name, subfolder=\"vae\")\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\n",
    "        config.architecture.model_name, subfolder=\"tokenizer\"\n",
    "    )\n",
    "    text_encoder = CLIPTextModel.from_pretrained(\n",
    "        config.architecture.model_name, subfolder=\"text_encoder\"\n",
    "    )\n",
    "    unet = UNet2DConditionModel.from_pretrained(\n",
    "        config.architecture.model_name, subfolder=\"unet\"\n",
    "    )\n",
    "    \n",
    "    if scheduler_type == \"DDPMScheduler\":\n",
    "        scheduler = DDPMScheduler.from_config(\n",
    "        config.architecture.scheduler, subfolder=\"scheduler\"\n",
    "        )\n",
    "    elif scheduler_type == \"LMSDiscreteScheduler\":\n",
    "        scheduler = LMSDiscreteScheduler.from_config(\n",
    "        config.architecture.scheduler, subfolder=\"scheduler\"\n",
    "        )\n",
    "\n",
    "    return vae, tokenizer, text_encoder, unet, scheduler\n",
    "\n",
    "\n",
    "def freeze_params(params):\n",
    "    for param in params:\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "def generate_image(prompt, neg_prompt, vae, tokenizer, text_encoder, unet, scheduler, config):\n",
    "    # Some settings\n",
    "    prompt = [prompt]\n",
    "    height = config.data.size                        # default height of Stable Diffusion\n",
    "    width = config.data.size                         # default width of Stable Diffusion\n",
    "    num_inference_steps = 30            # Number of denoising steps\n",
    "    guidance_scale = 8                # Scale for classifier-free guidance\n",
    "    generator = torch.manual_seed(32)   # Seed generator to create the inital latent noise\n",
    "    batch_size = 1\n",
    "\n",
    "    # Prep text \n",
    "    text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "    max_length = text_input.input_ids.shape[-1]\n",
    "    uncond_input = tokenizer(\n",
    "    [neg_prompt] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0] \n",
    "    text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "    # Prep Scheduler\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "    # Prep latents\n",
    "    latents = torch.randn(\n",
    "    (batch_size, unet.in_channels, height // 8, width // 8),\n",
    "    generator=generator,\n",
    "    )\n",
    "    latents = latents.to(torch_device)\n",
    "    latents = latents * scheduler.init_noise_sigma # Scaling (previous versions did latents = latents * self.scheduler.sigmas[0]\n",
    "\n",
    "    # Loop\n",
    "    with autocast(\"cuda\"):\n",
    "        for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "            # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "            latent_model_input = torch.cat([latents] * 2)\n",
    "            # Scale the latents (preconditioning):\n",
    "            latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "            # predict the noise residual\n",
    "            with torch.no_grad():\n",
    "                noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "            # perform guidance\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "            # compute the previous noisy sample x_t -> x_t-1\n",
    "            latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "    # scale and decode the image latents with vae\n",
    "    latents = 1 / 0.18215 * latents\n",
    "    with torch.no_grad():\n",
    "        image = vae.decode(latents).sample\n",
    "\n",
    "    # Display\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image * 255).round().astype(\"uint8\")\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "    return pil_images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce5f8f4-8e2e-465c-b918-97a6ec5fb5b0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fc06af-4280-4a49-b6f8-594cf1012640",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47f35b3-c6c5-4513-946c-f103bd93e15d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Python Dataset & Dataloaders\n",
    "object_templates = [\n",
    "    \"a photo of a {}\",\n",
    "    \"a rendering of a {}\",\n",
    "    \"a cropped photo of the {}\",\n",
    "    \"the photo of a {}\",\n",
    "    \"a photo of a clean {}\",\n",
    "    \"a photo of a dirty {}\",\n",
    "    \"a dark photo of the {}\",\n",
    "    \"a photo of my {}\",\n",
    "    \"a photo of the cool {}\",\n",
    "    \"a close-up photo of a {}\",\n",
    "    \"a bright photo of the {}\",\n",
    "    \"a cropped photo of a {}\",\n",
    "    \"a photo of the {}\",\n",
    "    \"a good photo of the {}\",\n",
    "    \"a photo of one {}\",\n",
    "    \"a close-up photo of the {}\",\n",
    "    \"a rendition of the {}\",\n",
    "    \"a photo of the clean {}\",\n",
    "    \"a rendition of a {}\",\n",
    "    \"a photo of a nice {}\",\n",
    "    \"a good photo of a {}\",\n",
    "    \"a photo of the nice {}\",\n",
    "    \"a photo of the small {}\",\n",
    "    \"a photo of the weird {}\",\n",
    "    \"a photo of the large {}\",\n",
    "    \"a photo of a cool {}\",\n",
    "    \"a photo of a small {}\",\n",
    "]\n",
    "\n",
    "style_templates = [\n",
    "    \"a painting in the style of {}\",\n",
    "    \"a rendering in the style of {}\",\n",
    "    \"a cropped painting in the style of {}\",\n",
    "    \"the painting in the style of {}\",\n",
    "    \"a clean painting in the style of {}\",\n",
    "    \"a dirty painting in the style of {}\",\n",
    "    \"a dark painting in the style of {}\",\n",
    "    \"a picture in the style of {}\",\n",
    "    \"a cool painting in the style of {}\",\n",
    "    \"a close-up painting in the style of {}\",\n",
    "    \"a bright painting in the style of {}\",\n",
    "    \"a cropped painting in the style of {}\",\n",
    "    \"a good painting in the style of {}\",\n",
    "    \"a close-up painting in the style of {}\",\n",
    "    \"a rendition in the style of {}\",\n",
    "    \"a nice painting in the style of {}\",\n",
    "    \"a small painting in the style of {}\",\n",
    "    \"a weird painting in the style of {}\",\n",
    "    \"a large painting in the style of {}\",\n",
    "]\n",
    "\n",
    "\n",
    "class TextualInversionDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_root,\n",
    "        tokenizer,\n",
    "        placeholder_token,\n",
    "        learnable_property=\"object\",\n",
    "        size=512,\n",
    "        repeats=100,\n",
    "        flip_p=0.5,\n",
    "    ):\n",
    "        self.data_root = data_root\n",
    "        self.tokenizer = tokenizer\n",
    "        self.learnable_property = learnable_property\n",
    "        self.size = size\n",
    "        self.placeholder_token = placeholder_token\n",
    "        self.flip_p = flip_p\n",
    "\n",
    "        self.image_paths = [\n",
    "            os.path.join(self.data_root, file_path)\n",
    "            for file_path in os.listdir(self.data_root)\n",
    "        ]\n",
    "\n",
    "        self.num_images = len(self.image_paths)\n",
    "        self._length = self.num_images * repeats\n",
    "\n",
    "        self.interpolation = Image.BICUBIC\n",
    "\n",
    "        self.templates = (\n",
    "            style_templates if learnable_property == \"style\" else object_templates\n",
    "        )\n",
    "        self.flip_transform = tfms.RandomHorizontalFlip(p=self.flip_p)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        text = random.choice(self.templates).format(self.placeholder_token)\n",
    "        text_input = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).input_ids[0]\n",
    "\n",
    "        image = Image.open(self.image_paths[i % self.num_images])\n",
    "        if not image.mode == \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        image = image.resize((self.size, self.size), resample=self.interpolation)\n",
    "        image = self.flip_transform(image)\n",
    "        image = np.array(image).astype(np.uint8)\n",
    "        image = (image / 127.5 - 1.0).astype(np.float32)\n",
    "        image_input = torch.from_numpy(image).permute(2, 0, 1)\n",
    "\n",
    "        return {\"text_input\": text_input, \"image_input\": image_input}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeea8014-8135-48cc-a32f-bedcf5190e40",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d7f289-b82f-43af-a9b0-2cba910c680b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare model artifacts for training\n",
    "vae, tokenizer, text_encoder, unet, scheduler = get_models(config, config.architecture.train_scheduler_type)\n",
    "\n",
    "token_ids = tokenizer.encode(config.data.initializer_token, add_special_tokens=False)\n",
    "if len(token_ids) > 1:\n",
    "    raise ValueError(\"The initializer token must be a single token.\")\n",
    "init_token_id = token_ids[0]\n",
    "placeholder_token_id = add_token(\n",
    "    config.data.placeholder_token, text_encoder, tokenizer, init_token_id\n",
    ")\n",
    "\n",
    "freeze_params(vae.parameters())\n",
    "freeze_params(unet.parameters())\n",
    "params_to_freeze = itertools.chain(\n",
    "    text_encoder.text_model.encoder.parameters(),\n",
    "    text_encoder.text_model.final_layer_norm.parameters(),\n",
    "    text_encoder.text_model.embeddings.position_embedding.parameters(),\n",
    ")\n",
    "freeze_params(params_to_freeze)\n",
    "\n",
    "vae = vae.to(torch_device)\n",
    "text_encoder = text_encoder.to(torch_device)\n",
    "unet = unet.to(torch_device)\n",
    "\n",
    "vae.eval()\n",
    "unet.eval()\n",
    "text_encoder.train()\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    text_encoder.get_input_embeddings().parameters(),\n",
    "    lr=config.optimization.learning_rate * config.optimization.batch_accum * config.optimization.batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b020bfe8-3e4c-485b-bb96-7eac454b5030",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dbdd86-b78b-4bbe-8efa-5604b0bd7cbf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = TextualInversionDataset(\n",
    "    data_root=config.data.image_dir,\n",
    "    tokenizer=tokenizer,\n",
    "    size=config.data.size,\n",
    "    placeholder_token=config.data.placeholder_token,\n",
    "    learnable_property=config.data.learnable_property,\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=config.optimization.batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "data_iterator = iter(train_dataloader)\n",
    "progress_bar = tqdm(range(config.optimization.num_train_steps))\n",
    "progress_bar.set_description(\"Steps\")\n",
    "\n",
    "for step in range(config.optimization.num_train_steps * config.optimization.batch_accum):\n",
    "    try:\n",
    "        batch = next(data_iterator)\n",
    "    except StopIteration:\n",
    "        train_dataloader = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=config.optimization.batch_size, shuffle=True\n",
    "        )\n",
    "    \n",
    "    latents = (\n",
    "        vae.encode(batch[\"image_input\"].to(torch_device)).latent_dist.sample().detach()\n",
    "    )\n",
    "    latents = latents * 0.18215\n",
    "\n",
    "    noise = torch.randn(latents.shape).to(latents.device)\n",
    "    bsz = latents.shape[0]\n",
    "    timesteps = torch.randint(\n",
    "        0, scheduler.config.num_train_timesteps, (bsz,), device=latents.device\n",
    "    ).long()\n",
    "    \n",
    "    noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
    "    encoder_hidden_states = text_encoder(batch[\"text_input\"].to(torch_device))[0]\n",
    "    noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "    loss = F.mse_loss(noise_pred, noise, reduction=\"none\").mean([1, 2, 3]).mean()\n",
    "    loss = loss / config.optimization.batch_accum\n",
    "    loss.backward()\n",
    "    \n",
    "    grads = text_encoder.get_input_embeddings().weight.grad\n",
    "    index_grads_to_zero = torch.arange(len(tokenizer)) != placeholder_token_id\n",
    "    grads.data[index_grads_to_zero, :] = grads.data[index_grads_to_zero, :].fill_(0)\n",
    "    \n",
    "    if (step + 1) % config.optimization.batch_accum == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        progress_bar.update(1)\n",
    "        logs = {\"loss\": loss.detach().item(), \"lr\": config.optimization.learning_rate * config.optimization.batch_accum * config.optimization.batch_size}\n",
    "        progress_bar.set_postfix(**logs)\n",
    "\n",
    "learned_embeds = text_encoder.get_input_embeddings().weight[placeholder_token_id]\n",
    "torch.save(\n",
    "    {config.data.placeholder_token: learned_embeds.detach().cpu()}, config.embedding_save_path.format(step)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede4bd81-4ef8-4969-970f-443223f59b54",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663997a9-ef7c-4f88-987a-32d3f34d8193",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c66a2c1-375e-4a7b-a9b1-8abb2d72db6d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vae, tokenizer, text_encoder, unet, scheduler = get_models(config, config.architecture.inference_scheduler_type)\n",
    "\n",
    "placeholder_token, embedding = list(torch.load(\"./kali_saved_embedding_19999.bin\").items())[0]\n",
    "add_token(placeholder_token, text_encoder, tokenizer, embedding)\n",
    "\n",
    "vae = vae.to(torch_device)\n",
    "text_encoder = text_encoder.to(torch_device)\n",
    "unet = unet.to(torch_device)\n",
    "\n",
    "vae.eval()\n",
    "unet.eval()\n",
    "text_encoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16613728-660f-400f-877b-5a0f54af0962",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Generate Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bb1b5b-4c0f-45ec-b91b-4366f9b8443f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generate_image(\"Oil painting of <kali-dog>\", \"\", vae, tokenizer, text_encoder, unet, scheduler, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a2a2a6-ebb9-4c61-897d-a6a9be25fe29",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}